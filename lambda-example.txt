
import json
import boto3
from datetime import datetime
from datetime import timedelta
import pandas as pd
import tweepy
import requests


def get_twitter_data():
    #Set twitter api params
    end = datetime.now()
    end = end - timedelta(minutes=1)
    start = end - timedelta(hours=1)
    number_of_tweets = 10
    hashtag = 'Colorado'
    query = '#'+hashtag+' lang:en -is:retweet'
    bearer = 'AAAAAAAAAAAAAAAAAAAAAGT7awEAAAAAWicvyzXTHSKXoCM%2BhSuKURxZUO0%3D4A6pcM8nwcGKB6QJ4tdUjsCYj3Mujz71b1I2PX4HZttlrUr3gd'
    client = tweepy.Client(bearer)
    
    #Get recent twitter data
    tweets = client.search_recent_tweets(query=query,
                                     start_time=start,
                                     end_time=end,
                                     tweet_fields = ["created_at", "text"],
                                     user_fields = ["name", "username", "location", "verified", "description"],
                                     max_results = number_of_tweets,
                                     expansions='author_id'
                                     )
                                     
    #Turn tweets into dataframe
    ids = []
    tweets_list = []
    creation_dates = []
    df = pd.DataFrame(columns = ['Tweet_Id', 'Tweet', 'Created_at' ])
    for tweet in tweets.data:
        ids.append(tweet.id)
        tweets_list.append(tweet.text)
        creation_dates.append(tweet.created_at.strftime("%d/%m/%Y %H:%M:%S"))
    df.Tweet_Id = ids
    df.Tweet = tweets_list
    df.Created_at = creation_dates
                                     
    return df

def get_s3_twitter_data():
    
    #Connect to s3 via boto3
    s3_client = boto3.client('s3',
                   aws_access_key_id='AKIA5RGT3KKY3FLP5IXU',
    aws_secret_access_key='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')
    #Store twitter csv file in lambda's temp storage
    s3_client.download_file(
        Bucket = 'vwbuckett',
        Key = 'twitter_data2.csv',
        Filename = '/tmp/download.csv')
      
    #Return dataframe   
    og_df = pd.read_csv('/tmp/download.csv')
    
    return og_df
    
    
def combine_dfs(og_df, new_df):
    #Concat dataframes
    final_df = pd.concat([new_df, og_df], axis = 0, ignore_index = True)
    #Store combined data in lambda temp folder
    final_df.to_csv('/tmp/upload.csv')
    
def lambda_handler(event, context):
    #Get new then old data
    new_df = get_twitter_data()
    og_df = get_s3_twitter_data()
    
    #Save file to local storage
    combine_dfs(og_df, new_df)
    
    #Reupload to s3 (hopefully)
    s3_client = boto3.client('s3',
                   aws_access_key_id='AKIA5RGT3KKY3FLP5IXU',
    aws_secret_access_key='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')
    
    s3_client.upload_file(
    Filename = '/tmp/upload.csv',
    Bucket = 'vwbuckett',
    Key = 'twitter_data2.csv'
    )

    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('WE MADE IT.')
    }
